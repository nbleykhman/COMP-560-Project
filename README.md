# COMP-560-Project


# Game-Playing AI: Chess Agent  

## Abstract  

This project focuses on developing an AI agent capable of playing chess through advanced search and decision-making techniques. The core approach involves implementing a depth-limited minimax algorithm with alpha-beta pruning to efficiently evaluate possible moves within a defined search depth. Minimax enables strategic play by simulating future board states and selecting optimal moves based on an evaluation function.  

To enhance the agent’s performance, we will explore Monte Carlo Tree Search, a probabilistic decision-making algorithm that dynamically balances exploration and exploitation. MCTS has been successfully applied in complex board games like Go, and we aim to adapt it for chess by incorporating domain-specific enhancements, such as heuristic-guided rollouts.  

As an extension, we plan to integrate Reinforcement Learning to further refine the agent’s strategic capabilities. This involves training the AI through self-play, allowing it to learn effective policies beyond our predefined heuristics. By incorporating these various techniques, the agent can develop more sophisticated strategies over time.  

Additionally, we aim to establish a competitive environment where different AI agents can compete against each other. This framework will facilitate benchmarking, performance evaluation, and optimization of different strategies. We will also attempt to put our agent against other AI chess AI to observe the relative performance of different decision making techniques. The project is structured to iteratively improve the agent’s decision-making abilities, focusing on algorithmic efficiency, adaptability, and strategic depth.  

Our GitHub repository will serve as a central hub for code, documentation, experiment results, and performance analysis.
